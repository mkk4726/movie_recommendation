"""
SVD ê¸°ë°˜ ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œ íŒŒì´í”„ë¼ì¸
"""
import pickle
import logging
from pathlib import Path
from typing import Tuple, Optional
from dataclasses import dataclass

import pandas as pd
from surprise import SVD, Dataset, Reader, accuracy
from surprise.model_selection import train_test_split

import sys
sys.path.append(str(Path(__file__).parent.parent.parent))
from data_scraping.common.data_loader import load_ratings_data, load_movie_data
from modeling.utils.data import filter_by_min_counts, preprocess_id_mapping, IDMapping

# Logger ì„¤ì •
logger = logging.getLogger(__name__)


@dataclass
class ModelConfig:
    """SVD ëª¨ë¸ ì„¤ì •"""
    n_factors: int = 50
    n_epochs: int = 20
    lr_all: float = 0.005
    reg_all: float = 0.02
    random_state: int = 42
    verbose: bool = True
    test_size: float = 0.2
    min_user_ratings: int = 30
    min_movie_ratings: int = 10
    rating_scale: Tuple[float, float] = (0.5, 5.0)


@dataclass
class EvaluationMetrics:
    """ëª¨ë¸ í‰ê°€ ì§€í‘œ"""
    train_rmse: float
    test_rmse: float
    train_mae: float
    test_mae: float
    user_overlap: float
    item_overlap: float
    
    def __str__(self):
        return f"""
=== í‰ê°€ ê²°ê³¼ ìš”ì•½ ===
Train RMSE: {self.train_rmse:.4f}
Test RMSE:  {self.test_rmse:.4f}
Train MAE:  {self.train_mae:.4f}
Test MAE:   {self.test_mae:.4f}

User Overlap: {self.user_overlap:.2f}%
Item Overlap: {self.item_overlap:.2f}%
"""


class SVDRecommenderPipeline:
    """
    SVD ê¸°ë°˜ ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸
    
    ì£¼ìš” ê¸°ëŠ¥:
    - ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
    - Train/Test ë¶„í• 
    - SVD ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    - ì˜í™” ì¶”ì²œ
    - ëª¨ë¸ ì €ì¥/ë¡œë“œ
    """
    
    def __init__(self, config: Optional[ModelConfig] = None):
        """
        Args:
            config: ëª¨ë¸ ì„¤ì • (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        """
        self.config = config or ModelConfig()
        
        # ë°ì´í„°
        self.df_raw = None
        self.df_filtered = None
        self.df_preprocessed = None
        self.id_mapping: Optional[IDMapping] = None
        
        # Surprise ë°ì´í„°ì…‹ ë° ëª¨ë¸
        self.surprise_data = None
        self.trainset = None
        self.testset = None
        self.svd_model = None
        
        # í‰ê°€ ì§€í‘œ
        self.metrics: Optional[EvaluationMetrics] = None
        
    def predict(self, user_id:str, movie_id:str) -> float:
        """
        íŠ¹ì • ì‚¬ìš©ìì™€ ì˜í™”ì— ëŒ€í•œ í‰ì  ì˜ˆì¸¡
        
        Args:
            user_id: ì‚¬ìš©ì ID
            movie_id: ì˜í™” ID
            
        Returns:
            ì˜ˆì¸¡ëœ í‰ì  (float)
        """
        if self.svd_model is None:
            raise ValueError("ëª¨ë¸ì„ ë¨¼ì € í•™ìŠµí•´ì£¼ì„¸ìš”. run_full_pipeline() ì‹¤í–‰ í•„ìš”")
        
        prediction = self.svd_model.predict(user_id, movie_id)
        return prediction.est
        
    def load_data(self, data_path: Optional[str] = None) -> pd.DataFrame:
        """
        í‰ì  ë°ì´í„°ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤.
        
        Args:
            data_path: ë°ì´í„° ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)
            
        Returns:
            ë¡œë”©ëœ í‰ì  ë°ì´í„°í”„ë ˆì„
        """
        logger.info("ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
        self.df_raw = load_ratings_data(data_path)
        
        logger.info("âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ")
        logger.info(f"  - ì‚¬ìš©ì ìˆ˜: {self.df_raw['user_id'].nunique():,}ëª…")
        logger.info(f"  - ì˜í™” ìˆ˜: {self.df_raw['movie_id'].nunique():,}ê°œ")
        logger.info(f"  - í‰ì  ìˆ˜: {len(self.df_raw):,}ê°œ")
        
        return self.df_raw
    
    def preprocess_data(self, df: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, IDMapping]:
        """
        ë°ì´í„° ì „ì²˜ë¦¬ (í•„í„°ë§ + ID ë§¤í•‘)
        
        Args:
            df: ì „ì²˜ë¦¬í•  ë°ì´í„°í”„ë ˆì„ (Noneì´ë©´ self.df_raw ì‚¬ìš©)
            
        Returns:
            ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ê³¼ ID ë§¤í•‘ ì •ë³´
        """
        if df is None:
            df = self.df_raw
            
        if df is None:
            raise ValueError("ë°ì´í„°ë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”. load_data() ì‹¤í–‰ í•„ìš”")
        
        logger.info("ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        
        # í•„í„°ë§
        self.df_filtered = filter_by_min_counts(
            df,
            min_user_ratings=self.config.min_user_ratings,
            min_movie_ratings=self.config.min_movie_ratings,
            verbose=True
        )
        
        # ID ë§¤í•‘
        self.df_preprocessed, self.id_mapping = preprocess_id_mapping(
            self.df_filtered,
            verbose=True
        )
        
        return self.df_preprocessed, self.id_mapping
    
    def prepare_surprise_dataset(self, df: Optional[pd.DataFrame] = None) -> Dataset:
        """
        Surprise ë¼ì´ë¸ŒëŸ¬ë¦¬ìš© ë°ì´í„°ì…‹ ì¤€ë¹„
        
        Args:
            df: ì‚¬ìš©í•  ë°ì´í„°í”„ë ˆì„ (Noneì´ë©´ self.df_preprocessed ì‚¬ìš©)
            
        Returns:
            Surprise Dataset ê°ì²´
        """
        if df is None:
            df = self.df_preprocessed
            
        if df is None:
            raise ValueError("ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”. preprocess_data() ì‹¤í–‰ í•„ìš”")
        
        logger.info("=== Surprise Dataset ì¤€ë¹„ ===")
        
        # Reader ê°ì²´ ìƒì„± (í‰ì  ë²”ìœ„ ì§€ì •)
        reader = Reader(rating_scale=self.config.rating_scale)
        
        # DataFrameì„ Surprise Datasetìœ¼ë¡œ ë³€í™˜
        self.surprise_data = Dataset.load_from_df(
            df[['user_id', 'movie_id', 'rating']],
            reader
        )
        
        logger.info("âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ")
        logger.info(f"  - ì´ í‰ì  ìˆ˜: {len(df):,}")
        logger.info(f"  - í‰ì  ë²”ìœ„: {self.config.rating_scale[0]} ~ {self.config.rating_scale[1]}")
        
        return self.surprise_data
    
    def split_train_test(self) -> Tuple:
        """
        Train/Test ë°ì´í„° ë¶„í• 
        
        Returns:
            (trainset, testset) íŠœí”Œ
        """
        if self.surprise_data is None:
            raise ValueError("Surprise ë°ì´í„°ì…‹ì„ ë¨¼ì € ì¤€ë¹„í•´ì£¼ì„¸ìš”. prepare_surprise_dataset() ì‹¤í–‰ í•„ìš”")
        
        logger.info(f"=== Train/Test Split (test_size={self.config.test_size}) ===")
        
        self.trainset, self.testset = train_test_split(
            self.surprise_data,
            test_size=self.config.test_size,
            random_state=self.config.random_state
        )
        
        logger.info("âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ")
        logger.info(f"  - Train set size: {self.trainset.n_ratings:,}")
        logger.info(f"  - Test set size: {len(self.testset):,}")
        logger.info("Train set í†µê³„:")
        logger.info(f"  - ì‚¬ìš©ì ìˆ˜: {self.trainset.n_users:,}")
        logger.info(f"  - ì˜í™” ìˆ˜: {self.trainset.n_items:,}")
        logger.info(f"  - í‰ì  ìˆ˜: {self.trainset.n_ratings:,}")
        logger.info(f"  - ì „ì²´ ì…€ ìˆ˜: {self.trainset.n_users * self.trainset.n_items:,}")
        sparsity = (1 - self.trainset.n_ratings / (self.trainset.n_users * self.trainset.n_items)) * 100
        logger.info(f"  - Train Sparsity: {sparsity:.2f}%")
        
        # User-Item Overlap í™•ì¸
        train_users = set(self.trainset._raw2inner_id_users.keys())
        train_items = set(self.trainset._raw2inner_id_items.keys())
        test_users = set([uid for (uid, _, _) in self.testset])
        test_items = set([iid for (_, iid, _) in self.testset])
        
        user_overlap = len(train_users & test_users) / len(test_users) * 100
        item_overlap = len(train_items & test_items) / len(test_items) * 100
        
        logger.info("[User/Item Overlap between Train and Test set]")
        logger.info(f"  - Test set ì‚¬ìš©ì ì¤‘, Train setì—ì„œ ë³¸ ì‚¬ìš©ì ë¹„ìœ¨: {user_overlap:.2f}% ({len(train_users & test_users)}/{len(test_users)})")
        logger.info(f"  - Test set ì•„ì´í…œ ì¤‘, Train setì—ì„œ ë³¸ ì•„ì´í…œ ë¹„ìœ¨: {item_overlap:.2f}% ({len(train_items & test_items)}/{len(test_items)})")
        
        return self.trainset, self.testset
    
    def train(self) -> SVD:
        """
        SVD ëª¨ë¸ í•™ìŠµ
        
        Returns:
            í•™ìŠµëœ SVD ëª¨ë¸
        """
        if self.trainset is None:
            raise ValueError("Train setì„ ë¨¼ì € ì¤€ë¹„í•´ì£¼ì„¸ìš”. split_train_test() ì‹¤í–‰ í•„ìš”")
        
        logger.info("=== SVD ëª¨ë¸ í•™ìŠµ ===")
        
        # SVD í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶œë ¥
        logger.info("SVD íŒŒë¼ë¯¸í„°:")
        logger.info(f"  - n_factors: {self.config.n_factors}")
        logger.info(f"  - n_epochs: {self.config.n_epochs}")
        logger.info(f"  - lr_all: {self.config.lr_all}")
        logger.info(f"  - reg_all: {self.config.reg_all}")
        logger.info(f"  - random_state: {self.config.random_state}")
        logger.info(f"  - verbose: {self.config.verbose}")
        
        logger.info("í•™ìŠµ ì‹œì‘...")
        
        # SVD ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
        self.svd_model = SVD(
            n_factors=self.config.n_factors,
            n_epochs=self.config.n_epochs,
            lr_all=self.config.lr_all,
            reg_all=self.config.reg_all,
            random_state=self.config.random_state,
            verbose=self.config.verbose
        )
        
        self.svd_model.fit(self.trainset)
        
        logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
        
        return self.svd_model
    
    def evaluate(self) -> EvaluationMetrics:
        """
        ëª¨ë¸ í‰ê°€ (Train/Test RMSE, MAE)
        
        Returns:
            í‰ê°€ ì§€í‘œê°€ ë‹´ê¸´ EvaluationMetrics ê°ì²´
        """
        if self.svd_model is None:
            raise ValueError("ëª¨ë¸ì„ ë¨¼ì € í•™ìŠµí•´ì£¼ì„¸ìš”. train() ì‹¤í–‰ í•„ìš”")
        
        logger.info("=== ëª¨ë¸ í‰ê°€ ===")
        
        # Test set í‰ê°€
        logger.info("Test set í‰ê°€:")
        test_predictions = self.svd_model.test(self.testset)
        test_rmse = accuracy.rmse(test_predictions, verbose=True)
        test_mae = accuracy.mae(test_predictions, verbose=True)
        
        # Train set í‰ê°€ (overfitting í™•ì¸ìš©)
        logger.info("Train set í‰ê°€:")
        train_testset = self.trainset.build_testset()
        train_predictions = self.svd_model.test(train_testset)
        train_rmse = accuracy.rmse(train_predictions, verbose=True)
        train_mae = accuracy.mae(train_predictions, verbose=True)
        
        # User-Item Overlap ê³„ì‚°
        train_users = set(self.trainset._raw2inner_id_users.keys())
        train_items = set(self.trainset._raw2inner_id_items.keys())
        test_users = set([uid for (uid, _, _) in self.testset])
        test_items = set([iid for (_, iid, _) in self.testset])
        
        user_overlap = len(train_users & test_users) / len(test_users) * 100
        item_overlap = len(train_items & test_items) / len(test_items) * 100
        
        # í‰ê°€ ì§€í‘œ ì €ì¥
        self.metrics = EvaluationMetrics(
            train_rmse=train_rmse,
            test_rmse=test_rmse,
            train_mae=train_mae,
            test_mae=test_mae,
            user_overlap=user_overlap,
            item_overlap=item_overlap
        )
        
        logger.info(str(self.metrics))
        
        # Overfitting ì²´í¬
        if test_rmse - train_rmse > 0.1:
            logger.warning("âš ï¸  ê²½ê³ : Test RMSEê°€ Train RMSEë³´ë‹¤ ìœ ì˜ë¯¸í•˜ê²Œ ë†’ìŠµë‹ˆë‹¤. Overfitting ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
        elif test_rmse - train_rmse < 0.05:
            logger.info("âœ… Trainê³¼ Test ì„±ëŠ¥ì´ ë¹„ìŠ·í•©ë‹ˆë‹¤. ì ì ˆí•œ ì¼ë°˜í™”ê°€ ì´ë£¨ì–´ì¡ŒìŠµë‹ˆë‹¤.")
        else:
            logger.info("âœ… Trainê³¼ Test ì„±ëŠ¥ ì°¨ì´ê°€ ì ì ˆí•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
        
        return self.metrics
    
    def recommend_for_user(
        self,
        user_id: str,
        df_movies: pd.DataFrame,
        n: int = 10
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        íŠ¹ì • ì‚¬ìš©ìì—ê²Œ ì˜í™” ì¶”ì²œ
        
        Args:
            user_id: ì¶”ì²œë°›ì„ ì‚¬ìš©ì ID
            df_movies: ì˜í™” ì •ë³´ ë°ì´í„°í”„ë ˆì„
            n: ì¶”ì²œí•  ì˜í™” ê°œìˆ˜
            
        Returns:
            (top_watched, recommendations) íŠœí”Œ
            - top_watched: ì‚¬ìš©ìê°€ ë†’ê²Œ í‰ê°€í•œ ì˜í™” Top N
            - recommendations: ì¶”ì²œ ì˜í™” Top N
        """
        if self.svd_model is None:
            raise ValueError("ëª¨ë¸ì„ ë¨¼ì € í•™ìŠµí•´ì£¼ì„¸ìš”. train() ì‹¤í–‰ í•„ìš”")
        
        if user_id not in self.df_preprocessed['user_id'].values:
            raise ValueError(f"ì‚¬ìš©ì ID '{user_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì‚¬ìš©ìê°€ ë³¸ ì˜í™”
        user_ratings = self.df_preprocessed[self.df_preprocessed['user_id'] == user_id]
        watched_movie_ids = set(user_ratings['movie_id'])
        
        # ì‚¬ìš©ìê°€ ë³´ì§€ ì•Šì€ ì˜í™”ì— ëŒ€í•´ ì˜ˆì¸¡
        all_movie_ids = set(self.df_preprocessed['movie_id'].unique())
        unseen_movie_ids = all_movie_ids - watched_movie_ids
        
        predictions = []
        for movie_id in unseen_movie_ids:
            pred = self.svd_model.predict(user_id, movie_id)
            predictions.append({
                'movie_id': movie_id,
                'predicted_rating': pred.est
            })
        
        # ì¶”ì²œ ëª©ë¡ ìƒì„±
        recommendations = pd.DataFrame(predictions)
        recommendations = pd.merge(recommendations, df_movies, on='movie_id', how='left')
        recommendations = recommendations.sort_values('predicted_rating', ascending=False).head(n)
        
        # ì‚¬ìš©ìê°€ ë†’ê²Œ í‰ê°€í•œ ì˜í™”
        top_watched = user_ratings.sort_values('rating', ascending=False).head(n)
        top_watched = pd.merge(top_watched, df_movies, on='movie_id', how='left')
        
        return top_watched, recommendations
    
    def save_model(self, filepath: str):
        """
        í•™ìŠµëœ ëª¨ë¸ì„ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            filepath: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        """
        if self.svd_model is None:
            raise ValueError("ì €ì¥í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. train() ë¨¼ì € ì‹¤í–‰ í•„ìš”")
        
        # ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
        model_data = {
            'config': self.config,
            'svd_model': self.svd_model,
            'id_mapping': self.id_mapping,
            'metrics': self.metrics,
            'df_preprocessed': self.df_preprocessed,
        }
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥
        logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {filepath}")
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        size_mb = Path(filepath).stat().st_size / (1024 * 1024)
        logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ! (í¬ê¸°: {size_mb:.2f} MB)")
        
    @classmethod
    def load_model(cls, filepath: str) -> 'SVDRecommenderPipeline':
        """
        ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œ
        
        Args:
            filepath: ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ë¡œë“œëœ SVDRecommenderPipeline ê°ì²´
        """
        logger.info(f"ğŸ“‚ ëª¨ë¸ ë¡œë”© ì¤‘: {filepath}")
        
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        # íŒŒì´í”„ë¼ì¸ ê°ì²´ ìƒì„±
        pipeline = cls(config=model_data['config'])
        pipeline.svd_model = model_data['svd_model']
        pipeline.id_mapping = model_data['id_mapping']
        pipeline.metrics = model_data['metrics']
        pipeline.df_preprocessed = model_data['df_preprocessed']
        
        logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        if pipeline.metrics:
            logger.info(f"  - Test RMSE: {pipeline.metrics.test_rmse:.4f}")
            logger.info(f"  - Test MAE: {pipeline.metrics.test_mae:.4f}")
        
        return pipeline
    
    def run_full_pipeline(self, data_path: Optional[str] = None) -> EvaluationMetrics:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë°ì´í„° ë¡œë”© -> ì „ì²˜ë¦¬ -> í•™ìŠµ -> í‰ê°€)
        
        Args:
            data_path: ë°ì´í„° ê²½ë¡œ
            
        Returns:
            í‰ê°€ ì§€í‘œ
        """
        logger.info("ğŸš€ SVD ì¶”ì²œ ì‹œìŠ¤í…œ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        logger.info("=" * 60)
        
        # 1. ë°ì´í„° ë¡œë”©
        self.load_data(data_path)
        
        # 2. ë°ì´í„° ì „ì²˜ë¦¬
        self.preprocess_data()
        
        # 3. Surprise ë°ì´í„°ì…‹ ì¤€ë¹„
        self.prepare_surprise_dataset()
        
        # 4. Train/Test ë¶„í• 
        self.split_train_test()
        
        # 5. ëª¨ë¸ í•™ìŠµ
        self.train()
        
        # 6. ëª¨ë¸ í‰ê°€
        metrics = self.evaluate()
        
        logger.info("=" * 60)
        logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        
        return metrics


# ê°„ë‹¨í•œ ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # Logger ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ëª¨ë¸ ì„¤ì •
    config = ModelConfig(
        n_factors=50,
        n_epochs=20,
        lr_all=0.005,
        reg_all=0.02,
        test_size=0.2,
        min_user_ratings=30,
        min_movie_ratings=10,
        verbose=True
    )
    
    # íŒŒì´í”„ë¼ì¸ ìƒì„± ë° ì‹¤í–‰
    pipeline = SVDRecommenderPipeline(config)
    metrics = pipeline.run_full_pipeline()
    
    # ëª¨ë¸ ì €ì¥
    model_path = Path(__file__).parent / 'pkls' / 'svd_model.pkl'
    pipeline.save_model(str(model_path))
    
    # ì˜í™” ë°ì´í„° ë¡œë“œ ë° ì¶”ì²œ
    df_movies = load_movie_data()
    
    # íŠ¹ì • ì‚¬ìš©ìì—ê²Œ ì¶”ì²œ
    user_id = pipeline.df_preprocessed['user_id'].iloc[0]
    top_watched, recommendations = pipeline.recommend_for_user(user_id, df_movies, n=5)
    
    logger.info("ğŸ¬ ìì£¼ ë³¸ ì˜í™” (ë‚´ê°€ ì§ì ‘ ë³¸ ì˜í™” ì¤‘ í‰ì  ìƒìœ„):")
    logger.info("\n" + top_watched[['movie_title', 'rating', 'title']].to_string(index=False))
    
    logger.info("âœ¨ ì¶”ì²œ ì˜í™” (ì•„ì§ ì•ˆ ë³¸ ì˜í™” ì¤‘ ì˜ˆìƒ í‰ì ì´ ë†’ì€ ìˆœ):")
    logger.info("\n" + recommendations[['title', 'predicted_rating', 'genre', 'year']].to_string(index=False))
